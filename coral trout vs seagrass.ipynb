{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f52cb2-bd1e-4176-9b72-a655e90520e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä½¿ç”¨æ•°æ®æ–‡ä»¶ï¼š /home/5013f3a6-a71a-4379-a32a-ece10ae96935/gladstone.csv\n",
      "ðŸ“˜ å·²å†™å…¥ï¼š gladstone_combined_frequencies.xlsx\n",
      "âœ… Done. è¾“å‡ºï¼š\n",
      " - gladstone_year_frequency.csv\n",
      " - gladstone_month_frequency.csv\n",
      " - gladstone_month_year_series.csv\n",
      " - gladstone_yearly_trend.png\n",
      " - gladstone_monthly_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# === One-Cell Final: combine frequencies + plots for gladstone.csv ===\n",
    "import re, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- 1) æ‰¾æ–‡ä»¶ï¼ˆå½“å‰ç›®å½•/ä¸Šä¸€çº§ + æ¨¡ç³ŠåŒ¹é…ï¼‰----\n",
    "TARGET_NAME = \"gladstone.csv\"\n",
    "\n",
    "try:\n",
    "    HERE = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    HERE = Path.cwd()\n",
    "\n",
    "cands = [\n",
    "    HERE / TARGET_NAME,\n",
    "    HERE / TARGET_NAME.strip(),\n",
    "    HERE.parent / TARGET_NAME,\n",
    "    HERE.parent / TARGET_NAME.strip(),\n",
    "]\n",
    "\n",
    "# å†åšæ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™/å¤šä½™ç©ºæ ¼ï¼‰\n",
    "def name_ok(s: str) -> bool:\n",
    "    s2 = re.sub(r\"\\s+\", \" \", s.lower()).strip()\n",
    "    return s2.endswith(\".csv\") and \"gladstone\" in s2\n",
    "\n",
    "for base in [HERE, HERE.parent]:\n",
    "    for p in base.glob(\"*.csv\"):\n",
    "        if name_ok(p.name):\n",
    "            cands.append(p)\n",
    "\n",
    "inp = next((p for p in cands if p.exists()), None)\n",
    "if inp is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"æ‰¾ä¸åˆ° {TARGET_NAME}ã€‚è¯·ç¡®è®¤æ–‡ä»¶åœ¨ä»¥ä¸‹ä»»ä¸€è·¯å¾„ï¼š\\n- {HERE}\\n- {HERE.parent}\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… ä½¿ç”¨æ•°æ®æ–‡ä»¶ï¼š\", inp)\n",
    "df = pd.read_csv(inp)\n",
    "\n",
    "# ---- 2) Presence è¿‡æ»¤ï¼ˆæœ‰å°±ç”¨ï¼Œæ²¡æœ‰å°±æŒ‰æ¯è¡Œ=1æ¬¡å‡ºçŽ°ï¼‰----\n",
    "def to_bool(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip().str.lower().isin({\"yes\",\"present\",\"1\",\"true\",\"y\"})\n",
    "\n",
    "mask = pd.Series(True, index=df.index)\n",
    "if \"H_UNINERVI\" in df.columns: mask &= to_bool(df[\"H_UNINERVI\"])\n",
    "if \"PRESENCE_A\" in df.columns: mask &= to_bool(df[\"PRESENCE_A\"])\n",
    "pres_cols = [c for c in df.columns if any(k in str(c).lower() for k in [\"presence\",\"occur\",\"count\",\"h_uninervi\"])]\n",
    "if pres_cols and (\"H_UNINERVI\" not in df.columns and \"PRESENCE_A\" not in df.columns):\n",
    "    c = pres_cols[0]\n",
    "    if pd.api.types.is_numeric_dtype(df[c]):\n",
    "        mask &= (pd.to_numeric(df[c], errors=\"coerce\").fillna(0) > 0)\n",
    "    else:\n",
    "        mask &= to_bool(df[c])\n",
    "\n",
    "data = df.loc[mask].copy()\n",
    "\n",
    "# ---- 3) Year/Month æž„é€  ----\n",
    "def normalize_month(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    if s.isdigit():\n",
    "        m = int(float(s)); return m if 1<=m<=12 else np.nan\n",
    "    s2 = s.capitalize()\n",
    "    month_map = {m:i for i,m in enumerate(\n",
    "        [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\n",
    "         \"September\",\"October\",\"November\",\"December\"], start=1)}\n",
    "    if s2 in month_map: return month_map[s2]\n",
    "    abbr = {m[:3]:i for m,i in month_map.items()}\n",
    "    return abbr.get(s2[:3], np.nan)\n",
    "\n",
    "year_col  = next((c for c in data.columns if str(c).lower()==\"year\"), None)\n",
    "month_col = next((c for c in data.columns if str(c).lower()==\"month\"), None)\n",
    "dt_col    = None\n",
    "if year_col is None or month_col is None:\n",
    "    dt_col = next((c for c in data.columns if any(k in str(c).lower() for k in [\"date\",\"time\",\"datetime\",\"timestamp\"])), None)\n",
    "\n",
    "if year_col and month_col:\n",
    "    data[\"Year\"]  = pd.to_numeric(data[year_col], errors=\"coerce\")\n",
    "    data[\"Month\"] = data[month_col].apply(normalize_month)\n",
    "elif dt_col:\n",
    "    data[\"_dt\"] = pd.to_datetime(data[dt_col], errors=\"coerce\")\n",
    "    data = data[~data[\"_dt\"].isna()].copy()\n",
    "    data[\"Year\"]  = data[\"_dt\"].dt.year\n",
    "    data[\"Month\"] = data[\"_dt\"].dt.month\n",
    "else:\n",
    "    gy = [c for c in data.columns if \"year\"  in str(c).lower()]\n",
    "    gm = [c for c in data.columns if \"month\" in str(c).lower()]\n",
    "    if gy: data[\"Year\"]  = pd.to_numeric(data[gy[0]], errors=\"coerce\")\n",
    "    if gm: data[\"Month\"] = data[gm[0]].apply(normalize_month)\n",
    "\n",
    "# ---- 4) é¢‘çŽ‡ç»Ÿè®¡ï¼ˆå¹´ã€æœˆã€é€æœˆ YYYY-MMï¼‰----\n",
    "yearly = (data.dropna(subset=[\"Year\"]).groupby(\"Year\")\n",
    "          .size().reset_index(name=\"Occurrences\").sort_values(\"Year\"))\n",
    "\n",
    "monthly = (data.dropna(subset=[\"Month\"]).groupby(\"Month\")\n",
    "           .size().reindex(range(1,13), fill_value=0).reset_index(name=\"Occurrences\"))\n",
    "monthly[\"Month_Name\"] = monthly[\"Month\"].apply(lambda m: pd.Timestamp(2000,int(m),1).strftime(\"%B\"))\n",
    "\n",
    "series = data.dropna(subset=[\"Year\",\"Month\"]).copy()\n",
    "series[\"Month_Period\"] = series.apply(lambda r: f\"{int(r['Year']):04d}-{int(r['Month']):02d}\", axis=1)\n",
    "series = (series.groupby(\"Month_Period\").size()\n",
    "          .reset_index(name=\"Occurrences\").sort_values(\"Month_Period\"))\n",
    "\n",
    "# ---- 5) å¯¼å‡ºï¼ˆåˆ°æ•°æ®åŒä¸€è·¯å¾„ï¼‰----\n",
    "out_dir = inp.parent\n",
    "prefix  = \"gladstone\"\n",
    "\n",
    "year_csv   = out_dir / f\"{prefix}_year_frequency.csv\"\n",
    "month_csv  = out_dir / f\"{prefix}_month_frequency.csv\"\n",
    "series_csv = out_dir / f\"{prefix}_month_year_series.csv\"\n",
    "yearly.to_csv(year_csv, index=False)\n",
    "monthly.to_csv(month_csv, index=False)\n",
    "series.to_csv(series_csv, index=False)\n",
    "\n",
    "# Excelï¼ˆè‹¥æœ‰ openpyxlï¼‰\n",
    "engine = None\n",
    "try:\n",
    "    import openpyxl  # noqa: F401\n",
    "    engine = \"openpyxl\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if engine:\n",
    "    xlsx = out_dir / f\"{prefix}_combined_frequencies.xlsx\"\n",
    "    with pd.ExcelWriter(xlsx, engine=engine) as w:\n",
    "        yearly.to_excel(w, sheet_name=\"Yearly\", index=False)\n",
    "        monthly.to_excel(w, sheet_name=\"MonthlyCalendar\", index=False)\n",
    "        series.to_excel(w, sheet_name=\"MonthlySeries\", index=False)\n",
    "        meta = pd.DataFrame({\"Source_File\":[str(inp.name)],\n",
    "                             \"Rows_Input\":[len(df)],\n",
    "                             \"Rows_After_Filter\":[len(data)]})\n",
    "        meta.to_excel(w, sheet_name=\"Meta\", index=False)\n",
    "    print(\"ðŸ“˜ å·²å†™å…¥ï¼š\", xlsx.name)\n",
    "else:\n",
    "    print(\"âš ï¸ æœªå®‰è£… openpyxlï¼Œå·²è·³è¿‡ xlsx å¯¼å‡ºï¼Œä»…ç”Ÿæˆ CSVã€‚\")\n",
    "\n",
    "# ---- 6) å›¾ï¼ˆmatplotlibï¼Œ1 å›¾/æ–‡ä»¶ï¼Œä¸è®¾é¢œè‰²é£Žæ ¼ï¼‰----\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f1 = plt.figure(figsize=(8,4.5))\n",
    "plt.plot(yearly[\"Year\"], yearly[\"Occurrences\"], marker=\"o\")\n",
    "plt.title(\"Yearly Occurrence Frequency\"); plt.xlabel(\"Year\"); plt.ylabel(\"Occurrences\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "(f1_path := out_dir / f\"{prefix}_yearly_trend.png\")\n",
    "f1.savefig(f1_path); plt.close(f1)\n",
    "\n",
    "f2 = plt.figure(figsize=(8,4.5))\n",
    "x = monthly[\"Month\"]; plt.bar(x, monthly[\"Occurrences\"])\n",
    "plt.xticks(x, monthly[\"Month_Name\"], rotation=45, ha=\"right\")\n",
    "plt.title(\"Monthly Occurrence Frequency (All Years Combined)\")\n",
    "plt.xlabel(\"Month\"); plt.ylabel(\"Occurrences\"); plt.tight_layout()\n",
    "(f2_path := out_dir / f\"{prefix}_monthly_distribution.png\")\n",
    "f2.savefig(f2_path); plt.close(f2)\n",
    "\n",
    "print(\"âœ… Done. è¾“å‡ºï¼š\")\n",
    "print(\" -\", year_csv.name)\n",
    "print(\" -\", month_csv.name)\n",
    "print(\" -\", series_csv.name)\n",
    "print(\" -\", f1_path.name)\n",
    "print(\" -\", f2_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610d1f7-62a6-4073-840f-1d693cceb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: make_halodule_frequencies.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= é…ç½® =========\n",
    "INPUT_XLSX = \"Townsville_Halodule_uninervis_2010-2018.xlsx\"  # ä½ çš„æ–‡ä»¶å\n",
    "MAKE_PLOTS = True\n",
    "OUTPUT_PREFIX = \"halodule\"\n",
    "# ======================\n",
    "\n",
    "def normalize_month(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    if s.isdigit():\n",
    "        m = int(float(s))\n",
    "        return m if 1 <= m <= 12 else np.nan\n",
    "    s_title = s.capitalize()\n",
    "    month_map = {m: i for i, m in enumerate(\n",
    "        [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\n",
    "         \"September\",\"October\",\"November\",\"December\"], start=1)}\n",
    "    if s_title in month_map: return month_map[s_title]\n",
    "    abbr_map = {name[:3]: idx for name, idx in month_map.items()}\n",
    "    return abbr_map.get(s_title[:3], np.nan)\n",
    "\n",
    "def presence_mask(df):\n",
    "    n = len(df)\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    def to_bool(col):\n",
    "        return df[col].astype(str).str.strip().str.lower().isin(\n",
    "            {\"yes\", \"present\", \"1\", \"true\"}\n",
    "        )\n",
    "    if \"H_UNINERVI\" in df.columns: mask &= to_bool(\"H_UNINERVI\")\n",
    "    if \"PRESENCE_A\" in df.columns: mask &= to_bool(\"PRESENCE_A\")\n",
    "    return mask\n",
    "\n",
    "def pick_excel_engine():\n",
    "    \"\"\"ä¼˜å…ˆ openpyxlï¼›æ²¡æœ‰å°±è¿”å›ž Noneï¼ˆè·³è¿‡å†™ xlsxï¼‰ã€‚\"\"\"\n",
    "    try:\n",
    "        import openpyxl  # noqa: F401\n",
    "        return \"openpyxl\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # å…¼å®¹è„šæœ¬/Notebook çš„è·¯å¾„å¤„ç†\n",
    "    try:\n",
    "        here = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        here = Path.cwd()\n",
    "\n",
    "    in_path = here / INPUT_XLSX\n",
    "    if not in_path.exists():\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ï¼š{in_path}\")\n",
    "\n",
    "    df = pd.read_excel(in_path)\n",
    "\n",
    "    # åŸºæœ¬åˆ—æ£€æŸ¥\n",
    "    if \"YEAR\" not in df.columns:\n",
    "        raise ValueError(\"ç¼ºå°‘ 'YEAR' åˆ—\")\n",
    "    if \"MONTH\" not in df.columns:\n",
    "        raise ValueError(\"ç¼ºå°‘ 'MONTH' åˆ—\")\n",
    "\n",
    "    # è¿‡æ»¤â€œå­˜åœ¨â€\n",
    "    data = df.loc[presence_mask(df)].copy()\n",
    "    data[\"Year\"] = pd.to_numeric(data[\"YEAR\"], errors=\"coerce\")\n",
    "    data[\"Month\"] = data[\"MONTH\"].apply(normalize_month)\n",
    "\n",
    "    # å¹´é¢‘çŽ‡\n",
    "    yearly = (data.dropna(subset=[\"Year\"])\n",
    "              .groupby(\"Year\").size()\n",
    "              .reset_index(name=\"Occurrences\")\n",
    "              .sort_values(\"Year\"))\n",
    "\n",
    "    # æœˆé¢‘çŽ‡ï¼ˆè·¨æ‰€æœ‰å¹´ä»½ï¼‰\n",
    "    monthly = (data.dropna(subset=[\"Month\"])\n",
    "               .groupby(\"Month\").size()\n",
    "               .reindex(range(1, 13), fill_value=0)\n",
    "               .reset_index(name=\"Occurrences\"))\n",
    "    monthly[\"Month_Name\"] = monthly[\"Month\"].apply(\n",
    "        lambda m: pd.Timestamp(2000, int(m), 1).strftime(\"%B\")\n",
    "    )\n",
    "\n",
    "    # é€æœˆåºåˆ— YYYY-MM\n",
    "    month_series = data.dropna(subset=[\"Year\", \"Month\"]).copy()\n",
    "    month_series[\"Month_Period\"] = month_series.apply(\n",
    "        lambda r: f\"{int(r['Year']):04d}-{int(r['Month']):02d}\", axis=1\n",
    "    )\n",
    "    month_series = (month_series.groupby(\"Month_Period\")\n",
    "                    .size().reset_index(name=\"Occurrences\")\n",
    "                    .sort_values(\"Month_Period\"))\n",
    "\n",
    "    # è¾“å‡ºè·¯å¾„\n",
    "    out_year_csv = here / f\"{OUTPUT_PREFIX}_year_frequency_2010-2018.csv\"\n",
    "    out_month_csv = here / f\"{OUTPUT_PREFIX}_month_frequency_all_years.csv\"\n",
    "    out_series_csv = here / f\"{OUTPUT_PREFIX}_month_year_frequency_2010-2018.csv\"\n",
    "    out_excel = here / f\"{OUTPUT_PREFIX}_frequencies_2010-2018.xlsx\"\n",
    "\n",
    "    # å†™ CSVï¼ˆçŽ¯å¢ƒæ— ä¾èµ–ï¼‰\n",
    "    yearly.to_csv(out_year_csv, index=False)\n",
    "    monthly.to_csv(out_month_csv, index=False)\n",
    "    month_series.to_csv(out_series_csv, index=False)\n",
    "\n",
    "    # å†™ Excelï¼ˆä¼˜å…ˆ openpyxlï¼›æ²¡æœ‰å°±è·³è¿‡ï¼‰\n",
    "    engine = pick_excel_engine()\n",
    "    if engine:\n",
    "        with pd.ExcelWriter(out_excel, engine=engine) as writer:\n",
    "            yearly.to_excel(writer, sheet_name=\"Yearly\", index=False)\n",
    "            monthly.to_excel(writer, sheet_name=\"MonthlyCalendar\", index=False)\n",
    "            month_series.to_excel(writer, sheet_name=\"MonthlySeries\", index=False)\n",
    "        print(\"âœ… Excel å†™å…¥å®Œæˆï¼š\", out_excel.name)\n",
    "    else:\n",
    "        print(\"âš ï¸ æœªå®‰è£… openpyxlï¼Œå·²è·³è¿‡ xlsx å¯¼å‡ºï¼Œä»…ç”Ÿæˆ CSVã€‚\"\n",
    "              \" å¦‚éœ€ xlsxï¼špip install openpyxl\")\n",
    "\n",
    "    print(\"âœ… å¹´é¢‘çŽ‡ CSVï¼š\", out_year_csv.name)\n",
    "    print(\"âœ… æœˆé¢‘çŽ‡ CSVï¼š\", out_month_csv.name)\n",
    "    print(\"âœ… é€æœˆåºåˆ— CSVï¼š\", out_series_csv.name)\n",
    "    print(f\"ç­›é€‰ä¸ºâ€œå­˜åœ¨â€çš„è®°å½•ï¼š{len(data)} / æ€»è¡Œæ•°ï¼š{len(df)}\")\n",
    "\n",
    "    # ç”»å›¾ï¼ˆmatplotlib è‡ªå¸¦ï¼Œä¸æŒ‡å®šé£Žæ ¼/é¢œè‰²ï¼‰\n",
    "    if MAKE_PLOTS:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # å¹´è¶‹åŠ¿\n",
    "        fig1 = plt.figure(figsize=(8, 4.5))\n",
    "        plt.plot(yearly[\"Year\"], yearly[\"Occurrences\"], marker=\"o\")\n",
    "        plt.title(\"Yearly Occurrence Frequency (2010â€“2018)\")\n",
    "        plt.xlabel(\"Year\"); plt.ylabel(\"Occurrences\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        fig1.savefig(here / f\"{OUTPUT_PREFIX}_yearly_trend.png\"); plt.close(fig1)\n",
    "\n",
    "        # æœˆåˆ†å¸ƒ\n",
    "        fig2 = plt.figure(figsize=(8, 4.5))\n",
    "        x = monthly[\"Month\"]\n",
    "        plt.bar(x, monthly[\"Occurrences\"])\n",
    "        plt.xticks(x, monthly[\"Month_Name\"], rotation=45, ha=\"right\")\n",
    "        plt.title(\"Monthly Occurrence Frequency (All Years Combined)\")\n",
    "        plt.xlabel(\"Month\"); plt.ylabel(\"Occurrences\")\n",
    "        plt.tight_layout()\n",
    "        fig2.savefig(here / f\"{OUTPUT_PREFIX}_monthly_distribution.png\"); plt.close(fig2)\n",
    "\n",
    "        print(\"ðŸ–¼ï¸ å›¾å·²ä¿å­˜ï¼š\",\n",
    "              f\"{OUTPUT_PREFIX}_yearly_trend.png, {OUTPUT_PREFIX}_monthly_distribution.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac96c2-2891-413d-86a1-1cd381a746b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2mix.csvï¼šå•ä¸ªå˜é‡(å« current_speed & wind_speed) â†’ æ•´åˆå˜é‡ ç›¸å…³çƒ­åŠ›å›¾ ===\n",
    "# å›¾ä¼šæ˜¾ç¤ºåœ¨ä»£ç ä¸‹æ–¹ï¼ŒåŒæ—¶å„è‡ªä¿å­˜ PNG/CSV\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- é…ç½® ----------\n",
    "CSV_NAME   = \"2mix.csv\"\n",
    "K          = 3   # æ•´åˆå˜é‡æ•°é‡ï¼š2 / 3 / 4\n",
    "OUT_PREFIX = \"2mix_corr_with_curr_wind\"\n",
    "# ------------------------\n",
    "\n",
    "# å·¥å…·ï¼šå®½æ¾åŒ¹é…åˆ—åï¼ˆå¿½ç•¥å¤§å°å†™/ç©ºæ ¼/è¿žå­—ç¬¦ï¼‰\n",
    "def _norm_map(cols): return {c.lower().replace(\" \", \"_\").replace(\"-\", \"_\"): c for c in cols}\n",
    "def _find(df, names):\n",
    "    m = _norm_map(df.columns)\n",
    "    for n in names:\n",
    "        k = n.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        if k in m: return m[k]\n",
    "    return None\n",
    "def _z(s):\n",
    "    mu, sd = s.mean(skipna=True), s.std(ddof=0, skipna=True)\n",
    "    if not np.isfinite(sd) or sd == 0: sd = 1.0\n",
    "    return (s - mu) / sd\n",
    "\n",
    "def plot_corr(df_vars: pd.DataFrame, title: str, outpng: Path):\n",
    "    corr = df_vars.corr(method=\"pearson\")\n",
    "    # æ˜¾ç¤º\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(corr.values, aspect=\"equal\", interpolation=\"nearest\", vmin=-1, vmax=1)\n",
    "    labels = list(corr.columns)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    cbar = plt.colorbar(); cbar.set_label(\"Pearson r\")\n",
    "    plt.title(title); plt.tight_layout(); plt.show()\n",
    "    # ä¿å­˜\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(corr.values, aspect=\"equal\", interpolation=\"nearest\", vmin=-1, vmax=1)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.colorbar(label=\"Pearson r\")\n",
    "    plt.title(title); plt.tight_layout()\n",
    "    plt.savefig(outpng, dpi=300); plt.close()\n",
    "    corr.to_csv(outpng.with_suffix(\".csv\"), index=True)\n",
    "    print(f\"âœ… Saved: {outpng.name} / {outpng.with_suffix('.csv').name}\")\n",
    "\n",
    "# 1) è¯»å–\n",
    "path = Path(CSV_NAME)\n",
    "if not path.exists(): raise FileNotFoundError(f\"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼š{path.resolve()}\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 2) â€”â€” å‡†å¤‡å•ä¸ªå˜é‡é›†åˆ â€”â€”ï¼ˆåŽ»æŽ‰ dateã€TOTAL_COTSã€MEAN_COTS_PER_TOWã€TOWSï¼›åŠ å…¥ current_speed & wind_speedï¼‰\n",
    "base_names = [\"eta\",\"salt\",\"temp\",\"u\",\"v\",\"wspeed_u\",\"wspeed_v\",\n",
    "              \"DIN\",\"DIP\",\"Dust\",\"PH\",\"Oxygen\",\"Chl\"]  # ä½ å›¾ä¸­çš„ä¸»ä½“å˜é‡\n",
    "picked = []\n",
    "for n in base_names:\n",
    "    col = _find(df, [n])\n",
    "    if col is not None: picked.append(col)\n",
    "    else: print(f\"âš ï¸ è·³è¿‡ï¼ˆæ‰¾ä¸åˆ°åˆ—ï¼‰ï¼š{n}\")\n",
    "\n",
    "# æž„é€ /èŽ·å– current_speed å’Œ wind_speed\n",
    "cur_col = _find(df, [\"current_speed\",\"current speed\"])\n",
    "wind_col= _find(df, [\"wind_speed\",\"wind speed\"])\n",
    "if cur_col is None:\n",
    "    u_col = _find(df, [\"u\"]); v_col = _find(df, [\"v\"])\n",
    "    if u_col is None or v_col is None:\n",
    "        raise KeyError(\"ç¼ºå°‘ current_speed åŠå…¶åˆ†é‡ u/vï¼Œæ— æ³•åŠ å…¥å›¾ã€‚\")\n",
    "    cur_series = np.hypot(pd.to_numeric(df[u_col], errors=\"coerce\"),\n",
    "                          pd.to_numeric(df[v_col], errors=\"coerce\"))\n",
    "    cur_col = \"current_speed(calc)\"\n",
    "else:\n",
    "    cur_series = pd.to_numeric(df[cur_col], errors=\"coerce\")\n",
    "\n",
    "if wind_col is None:\n",
    "    wu_col = _find(df, [\"wspeed_u\",\"wind_u\",\"wu\"])\n",
    "    wv_col = _find(df, [\"wspeed_v\",\"wind_v\",\"wv\"])\n",
    "    if wu_col is None or wv_col is None:\n",
    "        raise KeyError(\"ç¼ºå°‘ wind_speed åŠå…¶åˆ†é‡ wspeed_u/wspeed_vï¼Œæ— æ³•åŠ å…¥å›¾ã€‚\")\n",
    "    wind_series = np.hypot(pd.to_numeric(df[wu_col], errors=\"coerce\"),\n",
    "                           pd.to_numeric(df[wv_col], errors=\"coerce\"))\n",
    "    wind_col = \"wind_speed(calc)\"\n",
    "else:\n",
    "    wind_series = pd.to_numeric(df[wind_col], errors=\"coerce\")\n",
    "\n",
    "single_df = df[picked].apply(pd.to_numeric, errors=\"coerce\").copy()\n",
    "single_df[cur_col]  = cur_series.values\n",
    "single_df[wind_col] = wind_series.values\n",
    "single_df = single_df.dropna(axis=1, how=\"all\")\n",
    "plot_corr(single_df, \"Driver correlation matrix (single variables + current/wind speed)\",\n",
    "          Path(f\"{OUT_PREFIX}_single_with_curr_wind.png\"))\n",
    "\n",
    "# 3) â€”â€” æ•´åˆå˜é‡å›¾ï¼ˆK=2/3/4ï¼‰â€”â€”\n",
    "eta  = pd.to_numeric(df[_find(df, [\"eta\"])], errors=\"coerce\")\n",
    "salt = pd.to_numeric(df[_find(df, [\"salt\"])], errors=\"coerce\")\n",
    "temp = pd.to_numeric(df[_find(df, [\"temp\",\"temperature\"])], errors=\"coerce\")\n",
    "\n",
    "# ç›´æŽ¥å¤ç”¨ä¸Šé¢å¾—åˆ°çš„ cur_series / wind_series\n",
    "env_index     = pd.concat([_z(eta), _z(salt), _z(temp)], axis=1).mean(axis=1)\n",
    "temp_salt_idx = pd.concat([_z(temp), _z(salt)], axis=1).mean(axis=1)\n",
    "eta_z         = _z(eta)\n",
    "\n",
    "if K == 2:\n",
    "    comb = pd.DataFrame({\"current_speed\": cur_series, \"wind_speed\": wind_series})\n",
    "elif K == 3:\n",
    "    comb = pd.DataFrame({\"current_speed\": cur_series, \"wind_speed\": wind_series, \"env_index\": env_index})\n",
    "elif K == 4:\n",
    "    comb = pd.DataFrame({\"current_speed\": cur_series, \"wind_speed\": wind_series,\n",
    "                         \"temp_salt_index\": temp_salt_idx, \"eta_z\": eta_z})\n",
    "else:\n",
    "    raise ValueError(\"K åªèƒ½å– 2 / 3 / 4\")\n",
    "\n",
    "comb = comb.dropna(axis=1, how=\"all\")\n",
    "plot_corr(comb, f\"Driver correlation matrix (combined variables, K={K})\",\n",
    "          Path(f\"{OUT_PREFIX}_combined_K{K}.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
